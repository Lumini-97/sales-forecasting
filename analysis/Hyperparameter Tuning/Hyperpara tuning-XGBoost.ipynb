{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "import yaml\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config(config_path=r'C:\\Users\\MSI 11uc\\Documents\\ML\\CW\\Q5 Final\\Q5_sales_forecasting_final\\Q5_sales_forecasting\\config\\config.yaml'):\n",
    "    with open(config_path, 'r') as file:\n",
    "        return yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "target_column = config['target_column']\n",
    "train_data_path = config['data']['processed']['featured_train_data_output_path']\n",
    "test_data_path = config['data']['processed']['featured_test_data_output_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_xgb(df):\n",
    "    df_encoded = df.copy()\n",
    "    \n",
    "    label_encoders = {}\n",
    "    for column in ['store', 'item_dept', 'profile', 'size']:\n",
    "        le = LabelEncoder()\n",
    "        df_encoded[column] = le.fit_transform(df_encoded[column])\n",
    "        label_encoders[column] = le\n",
    "    \n",
    "    return df_encoded, label_encoders\n",
    "\n",
    "def bayesian_hyperparameter_tuning(train_df, test_df):\n",
    "    concat_df = pd.concat([train_df, test_df])\n",
    "    config = load_config()\n",
    "    \n",
    "    df_xgb, _ = preprocess_for_xgb(concat_df)\n",
    "    \n",
    "    Train = df_xgb.loc[df_xgb['date_id'] < config['data_cleaning']['train_date_id']]\n",
    "    \n",
    "    X_train = Train.drop(columns=[target_column, 'date_id'])\n",
    "    y_train = Train[target_column]\n",
    "    \n",
    "    # Define the hyperparameter search space\n",
    "    search_spaces = {\n",
    "        'n_estimators': Integer(100, 1000),\n",
    "        'max_depth': Integer(3, 10),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'subsample': Real(0.6, 1.0),\n",
    "        'colsample_bytree': Real(0.6, 1.0),\n",
    "        'min_child_weight': Integer(1, 10),\n",
    "        'gamma': Real(0, 5)\n",
    "    }\n",
    "    \n",
    "    # Initialize the XGBoost regressor\n",
    "    xgb = XGBRegressor(random_state=42)\n",
    "    \n",
    "    # Set up BayesSearchCV\n",
    "    bayes_search = BayesSearchCV(\n",
    "        estimator=xgb,\n",
    "        search_spaces=search_spaces,\n",
    "        n_iter=50,  # Number of parameter settings that are sampled\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=5,\n",
    "        verbose=2,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "    \n",
    "    # Fit BayesSearchCV\n",
    "    bayes_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Get the best parameters\n",
    "    best_params = bayes_search.best_params_\n",
    "    print(\"Best parameters found:\")\n",
    "    print(best_params)\n",
    "    \n",
    "    # Train the model with the best parameters\n",
    "    best_model = XGBRegressor(**best_params, random_state=42)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    return best_params, best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_data_path)\n",
    "test_df = pd.read_csv(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Best parameters found:\n",
      "OrderedDict([('colsample_bytree', 0.6), ('gamma', 5.0), ('learning_rate', 0.02318117860868824), ('max_depth', 9), ('min_child_weight', 9), ('n_estimators', 162), ('subsample', 0.6569299424463001)])\n"
     ]
    }
   ],
   "source": [
    "best_params, best_model = bayesian_hyperparameter_tuning(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "OrderedDict([('colsample_bytree', 0.6), ('gamma', 5.0), ('learning_rate', 0.02318117860868824), ('max_depth', 9), ('min_child_weight', 9), ('n_estimators', 162), ('subsample', 0.6569299424463001)])\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('colsample_bytree', 0.6),\n",
       "             ('gamma', 5.0),\n",
       "             ('learning_rate', 0.02318117860868824),\n",
       "             ('max_depth', 9),\n",
       "             ('min_child_weight', 9),\n",
       "             ('n_estimators', 162),\n",
       "             ('subsample', 0.6569299424463001)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     train_df = pd.read_csv(train_data_path)\n",
    "#     test_df = pd.read_csv(test_data_path)\n",
    "#     best_params, best_model = bayesian_hyperparameter_tuning(train_df, test_df)\n",
    "    \n",
    "#     # You can save the best_params to a file or print them here\n",
    "#     print(\"Best Hyperparameters:\")\n",
    "#     print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "7072",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
