{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor  # Correct import for XGBRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA  # Updated import for ARIMA\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Actual Preprocessing</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from a CSV file\n",
    "# Assuming the CSV file is named 'sales_data.csv'\n",
    "training_data = pd.read_csv(r'C:\\Users\\MSI 11uc\\Documents\\ML\\CW\\Q5_sales_forecasting\\data\\raw\\training_data.csv')\n",
    "test_data = pd.read_csv(r'C:\\Users\\MSI 11uc\\Documents\\ML\\CW\\Q5_sales_forecasting\\data\\raw\\test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 614098\n",
      "Number of duplicate rows: 1766\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_rows = training_data[training_data.duplicated()]\n",
    "\n",
    "# Print the duplicate rows, if any\n",
    "print(f\"Number of rows: {training_data.shape[0]}\")\n",
    "print(f\"Number of duplicate rows: {duplicate_rows.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicate rows\n",
    "training_data = training_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date_id            0\n",
      "item_dept          0\n",
      "item_qty           0\n",
      "net_sales          0\n",
      "store              0\n",
      "item               0\n",
      "invoice_num    21427\n",
      "day_of_week        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(training_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 612330\n"
     ]
    }
   ],
   "source": [
    "# Removing the outliers\n",
    "training_data = training_data[training_data['net_sales'] < 300000]\n",
    "training_data = training_data[training_data['item_qty'] < 800]\n",
    "\n",
    "print(f\"Number of rows: {training_data.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     date_id store  item_dept  item_qty     net_sales\n",
      "0 2021-11-01   ABC  Beverages   908.000  254007.79901\n",
      "1 2021-11-01   ABC    Grocery  2360.399  437286.62402\n",
      "2 2021-11-01   ABC  Household  1043.000  247336.26601\n",
      "3 2021-11-01   XYZ  Beverages   779.000  187931.76902\n",
      "4 2021-11-01   XYZ    Grocery  2974.060  489967.72300\n"
     ]
    }
   ],
   "source": [
    "### Groubpy the dataset based on date, net sales, item quantity\n",
    "grouped_df = training_data.groupby(['date_id', 'store', 'item_dept']).agg({\n",
    "    'item_qty': 'sum',\n",
    "    'net_sales': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Display the first few rows of the grouped dataframe\n",
    "print(grouped_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that each row is uniquely identified by 'store', 'item_dept', and 'date_id'\n",
    "grouped_df['primary_key'] = grouped_df['store'] + '|' + grouped_df['item_dept'] + '|' + grouped_df['date_id'].astype(str)\n",
    "\n",
    "# Check for duplicates based on primary keys\n",
    "if grouped_df.duplicated(subset=['primary_key']).any():\n",
    "    print(\"Warning: There are duplicate rows based on the primary keys.\")\n",
    "else:\n",
    "    print(\"All rows are uniquely identified by the primary keys.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The target variable is the daily sales quantity for each store and department\n",
    "target_variable = grouped_df[['store', 'item_dept', 'date_id', 'item_qty']]\n",
    "\n",
    "# Rename the target variable column for clarity\n",
    "target_variable.rename(columns={'item_qty': 'daily_sales_qty'}, inplace=True)\n",
    "\n",
    "# Display the target variable\n",
    "print(target_variable.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
